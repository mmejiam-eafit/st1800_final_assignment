prefix="../data"
dataset="news"
#dataset="news_reduced"
corpus="line.toml"
index="news"
#index="news_reduced"

stop-words = "lemur-stopwords.txt"
function-words = "function-words.txt"
punctuation = "sentence-boundaries/sentence-punctuation.txt"
start-exceptions = "sentence-boundaries/sentence-start-exceptions.txt"
end-exceptions = "sentence-boundaries/sentence-end-exceptions.txt"

[[analyzers]]
method = "ngram-word"
ngram = 1
    [[analyzers.filter]]
    type = "icu-tokenizer"

    [[analyzers.filter]]
    type = "lowercase"

    [[analyzers.filter]]
    type = "alpha"

    [[analyzers.filter]]
    type = "length"  
    min = 2
    max = 42

    [[analyzers.filter]]
    type = "list"
    file = "../data/lemur-stopwords.txt"

    [[analyzers.filter]]
    type = "empty-sentence"

#[[analyzers]]
#method = "ngram-word"
#ngram = 1
#filter = [{type = "icu-tokenizer"}, {type = "lowercase"}, {type = "alpha"}, {type = "length",  min = 2, max = 42}, {type = "list", file = "../data/lemur-stopwords.txt"}, {type = "empty-sentence"}]
    

#[ranker]
#method = "bm25"
#k1 = 1.2
#b = 0.75
#k3 = 500

[lda]
inference = "cvb" # or gibbs, or pargibbs
max-iters = 1000
alpha = 0.1
beta = 0.1
topics = 10
model-prefix = "lda-model"

[language-model]
arpa-file = "../data/english-sentences.arpa"
binary-file-prefix = "english-sentences-"